--- kernels/volk/volk_8u_x2_encodeframepolar_8u.h.orig
+++ kernels/volk/volk_8u_x2_encodeframepolar_8u.h
@@ -121,12 +121,12 @@ volk_8u_x2_encodeframepolar_8u_u_ssse3(unsigned char* frame, unsigned char* temp
           r_temp1 = _mm_loadu_si128((__m128i *) temp_ptr);
           temp_ptr += 16;
 
-          shifted = _mm_bsrli_si128(r_temp0, 1);
+          shifted = _mm_srli_si128(r_temp0, 1);
           shifted = _mm_and_si128(shifted, mask_stage1);
           r_temp0 = _mm_xor_si128(shifted, r_temp0);
           r_temp0 = _mm_shuffle_epi8(r_temp0, shuffle_separate);
 
-          shifted = _mm_bsrli_si128(r_temp1, 1);
+          shifted = _mm_srli_si128(r_temp1, 1);
           shifted = _mm_and_si128(shifted, mask_stage1);
           r_temp1 = _mm_xor_si128(shifted, r_temp1);
           r_temp1 = _mm_shuffle_epi8(r_temp1, shuffle_separate);
@@ -170,7 +170,7 @@ volk_8u_x2_encodeframepolar_8u_u_ssse3(unsigned char* frame, unsigned char* temp
     // shuffle once for bit-reversal.
     r_temp0 = _mm_shuffle_epi8(r_temp0, shuffle_stage4);
 
-    shifted = _mm_bsrli_si128(r_temp0, 8);
+    shifted = _mm_srli_si128(r_temp0, 8);
     shifted = _mm_and_si128(shifted, mask_stage4);
     r_frame0 = _mm_xor_si128(shifted, r_temp0);
 
@@ -178,15 +178,15 @@ volk_8u_x2_encodeframepolar_8u_u_ssse3(unsigned char* frame, unsigned char* temp
     r_temp0 = _mm_loadu_si128((__m128i*) temp_ptr);
     temp_ptr += 16;
 
-    shifted = _mm_bsrli_si128(r_frame0, 4);
+    shifted = _mm_srli_si128(r_frame0, 4);
     shifted = _mm_and_si128(shifted, mask_stage3);
     r_frame0 = _mm_xor_si128(shifted, r_frame0);
 
-    shifted = _mm_bsrli_si128(r_frame0, 2);
+    shifted = _mm_srli_si128(r_frame0, 2);
     shifted = _mm_and_si128(shifted, mask_stage2);
     r_frame0 = _mm_xor_si128(shifted, r_frame0);
 
-    shifted = _mm_bsrli_si128(r_frame0, 1);
+    shifted = _mm_srli_si128(r_frame0, 1);
     shifted = _mm_and_si128(shifted, mask_stage1);
     r_frame0 = _mm_xor_si128(shifted, r_frame0);
 
@@ -243,12 +243,12 @@ volk_8u_x2_encodeframepolar_8u_a_ssse3(unsigned char* frame, unsigned char* temp
           r_temp1 = _mm_load_si128((__m128i *) temp_ptr);
           temp_ptr += 16;
 
-          shifted = _mm_bsrli_si128(r_temp0, 1);
+          shifted = _mm_srli_si128(r_temp0, 1);
           shifted = _mm_and_si128(shifted, mask_stage1);
           r_temp0 = _mm_xor_si128(shifted, r_temp0);
           r_temp0 = _mm_shuffle_epi8(r_temp0, shuffle_separate);
 
-          shifted = _mm_bsrli_si128(r_temp1, 1);
+          shifted = _mm_srli_si128(r_temp1, 1);
           shifted = _mm_and_si128(shifted, mask_stage1);
           r_temp1 = _mm_xor_si128(shifted, r_temp1);
           r_temp1 = _mm_shuffle_epi8(r_temp1, shuffle_separate);
@@ -292,7 +292,7 @@ volk_8u_x2_encodeframepolar_8u_a_ssse3(unsigned char* frame, unsigned char* temp
     // shuffle once for bit-reversal.
     r_temp0 = _mm_shuffle_epi8(r_temp0, shuffle_stage4);
 
-    shifted = _mm_bsrli_si128(r_temp0, 8);
+    shifted = _mm_srli_si128(r_temp0, 8);
     shifted = _mm_and_si128(shifted, mask_stage4);
     r_frame0 = _mm_xor_si128(shifted, r_temp0);
 
@@ -300,15 +300,15 @@ volk_8u_x2_encodeframepolar_8u_a_ssse3(unsigned char* frame, unsigned char* temp
     r_temp0 = _mm_load_si128((__m128i*) temp_ptr);
     temp_ptr += 16;
 
-    shifted = _mm_bsrli_si128(r_frame0, 4);
+    shifted = _mm_srli_si128(r_frame0, 4);
     shifted = _mm_and_si128(shifted, mask_stage3);
     r_frame0 = _mm_xor_si128(shifted, r_frame0);
 
-    shifted = _mm_bsrli_si128(r_frame0, 2);
+    shifted = _mm_srli_si128(r_frame0, 2);
     shifted = _mm_and_si128(shifted, mask_stage2);
     r_frame0 = _mm_xor_si128(shifted, r_frame0);
 
-    shifted = _mm_bsrli_si128(r_frame0, 1);
+    shifted = _mm_srli_si128(r_frame0, 1);
     shifted = _mm_and_si128(shifted, mask_stage1);
     r_frame0 = _mm_xor_si128(shifted, r_frame0);
 
